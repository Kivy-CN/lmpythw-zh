# 练习 18：性能测量

在本练习中，您将学习使用多种工具来分析您创建的数据结构和算法的性能。为了使这个介绍专注并且简洁，我们将查看练习 16 中的`sorted.py`算法的性能，然后在视频中，我会分析我们迄今为止所做的所有数据结构的性能。

性能分析和调优是我最喜欢的计算机编程活动之一。在看电视的时候，我是那个手里拿着一团缠着的绳子的人，并且只打算把它解开，直到它很好并且有序。我喜欢探究复杂的奥秘，代码性能是最复杂的奥秘之一。有一些很好的并且实用的工具，用于分析代码的性能，使之比调试更好。

编码时不要试图实现性能改进，除非它们是显而易见的。我更喜欢使我的代码的初始版本保持极其简单和朴素，以便我可以确保它正常工作。然后，一旦它运行良好，但也许很慢，我启动我的分析工具，并开始寻找方法使其更快，而不降低稳定性。最后一部分是关键，因为许多程序员觉得如果能使代码更快，那么可以降低代码的稳定性和安全性。

## 工具

在本练习中，我们将介绍许多有用的 Python 工具，以及一些改进任何代码性能的一般策略。我们将使用的工具有：

+   [`timeit`](https://docs.python.org/3/library/timeit.html)
+   [`cProfile` 和 `profile`](https://docs.python.org/2/library/profile.html)

在继续之前，请确保安装任何需要安装的软件。然后获取`sorted.py`和`test_sorting.py`文件的副本，以便我们可以将这些工具应用到这些算法中。

### `timeit`

`timeit`模块不是非常有用。它所做的就是接受字符串形式的 Python 代码，并使用一些时间运行它。您不能传递函数引用，`.py`文件或除字符串之外的任何内容。我们可以在`test_sorting.py`的结尾，测试`test_bubble_sort`函数需要多长时间：

```py
if __name__ == '__main__':
    import timeit
    print(timeit.timeit("test_bubble_sort()", setup="from __main__ import test_bubble_sort"))
```

它也不会产生有用的测量或任何信息，为什么某些东西可能很慢。我们需要一种方式来衡量代码运行的时间长短，这样做太笨重了，无法使用。

### `cProfile`和`profile`

接下来的两个工具，对于测量代码的性能来说更为有用。我建议使用`cProfile`来分析代码的运行时间，并且当你在分析中需要更多的灵活性时，保存`profile`。为了对你的测试运行`cProfile`，请更改`test_sorting.py`文件的末尾，来简单地运行测试函数：

```py
if __name__ == '__main__':
    test_bubble_sort()
    test_merge_sort()
```

并将`max_numbers`更改为大约 800，或足够大的数字，以便您可以测量效果。一旦你完成了，然后在你的代码上运行`cProfile`：

```
$ python -m cProfile -s cumtime test_sorting.py | grep sorting.py
```

我使用了`| grep sorted.py`，只是将输出缩小到我关心的文件，但删除该部分命令可以查看完整的输出。我在相当快的电脑上获得的 800 个数字的结果是：

```
  ncalls  tottime  percall  cumtime  percall filename:lineno(function)
       1    0.000    0.000    0.145    0.145 test_sorting.py:1(<module>)
       1    0.000    0.000    0.128    0.128 test_sorting.py:25(test_bubble_sort)
       1    0.125    0.125    0.125    0.125 sorting.py:6(bubble_sort)
       1    0.000    0.000    0.009    0.009 sorting.py:1(<module>)
       1    0.000    0.000    0.008    0.008 test_sorting.py:33(test_merge_sort)
       2    0.001    0.000    0.006    0.003 test_sorting.py:7(random_list)
       1    0.000    0.000    0.005    0.005 sorting.py:37(merge_sort)
  1599/1    0.001    0.000    0.005    0.005 sorting.py:47(merge_node)
7500/799    0.004    0.000    0.004    0.000 sorting.py:72(merge)
     799    0.001    0.000    0.001    0.000 sorting.py:27(count)
       2    0.000    0.000    0.000    0.000 test_sorting.py:14(is_sorted)
```

我在顶部添加了标题，以便你看到输出表示什么。每个标题的意思是：

> `ncalls`

> 该函数的调用次数

> `tottime`

> 总执行时间

> `percall`

> 函数每个调用的总时间

> `cumtime`

> 函数的累计时间

> `percall`

> 每个调用的累计时间

> `filename:lineno(function)`

> 名称、行号和涉及到的函数

那些标题名称也可以使用`-s`参数来获取。然后，我们可以对此输出进行快速分析：

`bubble_sort`被调用一次，但`merge_node`被调用了 1599 次，并且`merge `甚至调用了 7500 次。这是因为`merge_node`和`merge`是递归的，所以对一个有 800 个元素的随机列表排序时，他们会产生大量的调用。

即使`bubble_sort`不像`merge`或`merge_node`一样被频繁调用，它也是很慢的。这符合两种算法的性能预期。归并排序的最坏情况是`O(nlogn)`，但是对于冒泡排序，它是`O(n^2)`。如果你有 800 个元素，那么`800 * log(800)`约为 5347，而`800^2`是 640000！这些数字不一定会转化为这些算法运行的精确秒数，但它们确实会转化为相对比较。

`count`函数被调用 799 次，这最有可能是巨大的浪费。我们实现的`DoubleLinkedList`并不追踪元素的数量，而是必须在每一次你想知道数量的时候遍历这个列表。我们在这里的`count`函数中使用相同的方法，并且导致了整个列表中的 800 个元素的 799 次遍历。将`max_numbers`更改为 600 或 500 在这里查看规律。注意在我们的实现中，`count `是否运行了`n-1`次？这意味着我们遍历了几乎所有 800 个元素。

现在让我们查看，`dllist.py`如何影响其性能：

同样，我已经添加了标题，以便您可以看到发生了什么。在这种情况下，您可以看到，与`merge`，`merge_node`和`count`函数相比，`dllist.py`函数不会影响性能。这是很重要的，因为大多数程序员将运行优化`DoubleLinkedList`数据结构，但在`merge_sort`实现中可以获得更大的收益，并且完全可以避免使用`bubble_sort`。始终以最小的努力获得最大的改进。

